{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd3d44-b20d-47d6-b718-04b06a907765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on page 1: HTTPSConnectionPool(host='example.com', port=443): Max retries exceeded with url: /api/endpoint?page=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))\n",
      "\n",
      "Error on page 2: HTTPSConnectionPool(host='example.com', port=443): Max retries exceeded with url: /api/endpoint?page=2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))\n",
      "\n",
      "Error on page 3: HTTPSConnectionPool(host='example.com', port=443): Max retries exceeded with url: /api/endpoint?page=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1032)')))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generic Paginated API Scraper\n",
    "--------------------------------\n",
    "This script fetches paginated data from a REST API endpoint\n",
    "and progressively saves results to a CSV file.\n",
    "\n",
    "Designed for:\n",
    "- Large datasets\n",
    "- Crash recovery (resume logic)\n",
    "- Safe incremental saving\n",
    "\n",
    "Replace the placeholders below with your own API details.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================\n",
    "\n",
    "# ðŸ”¹ Replace with your own API endpoint\n",
    "BASE_URL = \"https://example.com/api/endpoint\"  # <-- ADD YOUR API URL HERE\n",
    "\n",
    "# ðŸ”¹ Total number of pages available in the API\n",
    "TOTAL_PAGES = 1000  # <-- UPDATE THIS VALUE\n",
    "\n",
    "# ðŸ”¹ Output CSV filename\n",
    "SAVE_FILE = \"output_data.csv\"\n",
    "\n",
    "# ðŸ”¹ Custom headers (modify if needed)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"  # <-- Customize if required\n",
    "}\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# RESUME LOGIC (Optional but recommended)\n",
    "# ==========================================================\n",
    "\n",
    "start_page = 1\n",
    "\n",
    "if os.path.exists(SAVE_FILE):\n",
    "    existing_df = pd.read_csv(SAVE_FILE)\n",
    "    rows_existing = len(existing_df)\n",
    "\n",
    "    # âš ï¸ Assumes fixed number of records per page\n",
    "    # Adjust divisor if your API returns a different page size\n",
    "    records_per_page = 10  # <-- UPDATE IF DIFFERENT\n",
    "    pages_done = rows_existing // records_per_page\n",
    "\n",
    "    start_page = pages_done + 1\n",
    "    print(f\"Resuming from page {start_page}\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN LOOP\n",
    "# ==========================================================\n",
    "\n",
    "for page in range(start_page, TOTAL_PAGES + 1):\n",
    "    try:\n",
    "        # API query parameters\n",
    "        params = {\n",
    "            \"page\": page  # <-- MODIFY PARAMS IF NEEDED\n",
    "        }\n",
    "\n",
    "        response = requests.get(\n",
    "            BASE_URL,\n",
    "            params=params,\n",
    "            headers=headers,\n",
    "            timeout=15  # Prevents hanging requests\n",
    "        )\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # ðŸ”¹ Modify field names according to your API structure\n",
    "        for item in data[\"data\"]:  # <-- Adjust JSON path if needed\n",
    "            rows.append({\n",
    "                \"Field_1\": item.get(\"field_1\", \"\"),  # <-- CHANGE\n",
    "                \"Field_2\": item.get(\"field_2\", \"\"),  # <-- CHANGE\n",
    "                \"Field_3\": item.get(\"field_3\", \"\"),  # <-- CHANGE\n",
    "            })\n",
    "\n",
    "        df_page = pd.DataFrame(rows)\n",
    "\n",
    "        # Append to CSV safely\n",
    "        df_page.to_csv(\n",
    "            SAVE_FILE,\n",
    "            mode='a',\n",
    "            header=not os.path.exists(SAVE_FILE),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        # Single-line progress display\n",
    "        sys.stdout.write(f\"\\rPage {page} / {TOTAL_PAGES}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Optional polite delay\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on page {page}: {e}\")\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "print(\"\\nDONE âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4963a1-6d69-49b7-b05b-72a71e2180fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
